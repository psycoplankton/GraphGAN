{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"/home/student/Vansh/GraphGAN/CA-GrQc Dataset/CA-GrQc_train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NodeIDfrom</th>\n",
       "      <th>NodeIDto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4095</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3213</td>\n",
       "      <td>2059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1882</td>\n",
       "      <td>3269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4897</td>\n",
       "      <td>2661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2621</td>\n",
       "      <td>1718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13041</th>\n",
       "      <td>830</td>\n",
       "      <td>5103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13042</th>\n",
       "      <td>1842</td>\n",
       "      <td>3805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13043</th>\n",
       "      <td>3468</td>\n",
       "      <td>1758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13044</th>\n",
       "      <td>4889</td>\n",
       "      <td>3784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13045</th>\n",
       "      <td>2796</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13046 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NodeIDfrom  NodeIDto\n",
       "0            4095       546\n",
       "1            3213      2059\n",
       "2            1882      3269\n",
       "3            4897      2661\n",
       "4            2621      1718\n",
       "...           ...       ...\n",
       "13041         830      5103\n",
       "13042        1842      3805\n",
       "13043        3468      1758\n",
       "13044        4889      3784\n",
       "13045        2796       504\n",
       "\n",
       "[13046 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset,\n",
    "                sep = '\\t',\n",
    "                names = [\"NodeIDfrom\", \"NodeIDto\"],\n",
    "                )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5119"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = nx.from_pandas_edgelist(df = df,\n",
    "                             source = \"NodeIDfrom\",\n",
    "                             target = \"NodeIDto\",\n",
    "                             create_using=nx.Graph())\n",
    "len(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_alias_table(area_ratio):\n",
    "    \"\"\"\n",
    "\n",
    "    :param area_ratio: sum(area_ratio)=1\n",
    "    :return: accept,alias\n",
    "    \"\"\"\n",
    "    l = len(area_ratio)\n",
    "    accept, alias = [0] * l, [0] * l\n",
    "    small, large = [], []\n",
    "    area_ratio_ = np.array(area_ratio) * l\n",
    "    for i, prob in enumerate(area_ratio_):\n",
    "        if prob < 1.0:\n",
    "            small.append(i)\n",
    "        else:\n",
    "            large.append(i)\n",
    "\n",
    "    while small and large:\n",
    "        small_idx, large_idx = small.pop(), large.pop()\n",
    "        accept[small_idx] = area_ratio_[small_idx]\n",
    "        alias[small_idx] = large_idx\n",
    "        area_ratio_[large_idx] = area_ratio_[large_idx] - \\\n",
    "                                 (1 - area_ratio_[small_idx])\n",
    "        if area_ratio_[large_idx] < 1.0:\n",
    "            small.append(large_idx)\n",
    "        else:\n",
    "            large.append(large_idx)\n",
    "\n",
    "    while large:\n",
    "        large_idx = large.pop()\n",
    "        accept[large_idx] = 1\n",
    "    while small:\n",
    "        small_idx = small.pop()\n",
    "        accept[small_idx] = 1\n",
    "\n",
    "    return accept, alias\n",
    "\n",
    "\n",
    "def alias_sample(accept, alias):\n",
    "    \"\"\"\n",
    "\n",
    "    :param accept:\n",
    "    :param alias:\n",
    "    :return: sample index\n",
    "    \"\"\"\n",
    "    N = len(accept)\n",
    "    i = int(np.random.random() * N)\n",
    "    r = np.random.random()\n",
    "    if r < accept[i]:\n",
    "        return i\n",
    "    else:\n",
    "        return alias[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nxgraph(graph):\n",
    "    node2idx = {}\n",
    "    idx2node = []\n",
    "    node_size = 0\n",
    "    for node in graph.nodes():\n",
    "        node2idx[node] = node_size\n",
    "        idx2node.append(node)\n",
    "        node_size += 1\n",
    "    return idx2node, node2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 11:04:47.527474: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.layers import Embedding, Input, Lambda\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "\n",
    "\n",
    "def line_loss(y_true, y_pred):\n",
    "    return -K.mean(K.log(K.sigmoid(y_true * y_pred)))\n",
    "\n",
    "\n",
    "def create_model(numNodes, embedding_size, order='second'):\n",
    "    v_i = Input(shape=(1,))\n",
    "    v_j = Input(shape=(1,))\n",
    "\n",
    "    first_emb = Embedding(numNodes, embedding_size, name='first_emb')\n",
    "    second_emb = Embedding(numNodes, embedding_size, name='second_emb')\n",
    "    context_emb = Embedding(numNodes, embedding_size, name='context_emb')\n",
    "\n",
    "    v_i_emb = first_emb(v_i)\n",
    "    v_j_emb = first_emb(v_j)\n",
    "\n",
    "    v_i_emb_second = second_emb(v_i)\n",
    "    v_j_context_emb = context_emb(v_j)\n",
    "\n",
    "    first = Lambda(lambda x: tf.reduce_sum(\n",
    "        x[0] * x[1], axis=-1, keepdims=False), name='first_order')([v_i_emb, v_j_emb])\n",
    "    second = Lambda(lambda x: tf.reduce_sum(\n",
    "        x[0] * x[1], axis=-1, keepdims=False), name='second_order')([v_i_emb_second, v_j_context_emb])\n",
    "\n",
    "    if order == 'first':\n",
    "        output_list = [first]\n",
    "    elif order == 'second':\n",
    "        output_list = [second]\n",
    "    else:\n",
    "        output_list = [first, second]\n",
    "\n",
    "    model = Model(inputs=[v_i, v_j], outputs=output_list)\n",
    "\n",
    "    return model, {'first': first_emb, 'second': second_emb}\n",
    "\n",
    "\n",
    "class LINE:\n",
    "    def __init__(self, graph, embedding_size=8, negative_ratio=5, order='second', ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param graph:\n",
    "        :param embedding_size:\n",
    "        :param negative_ratio:\n",
    "        :param order: 'first','second','all'\n",
    "        \"\"\"\n",
    "        if order not in ['first', 'second', 'all']:\n",
    "            raise ValueError('mode must be fisrt,second,or all')\n",
    "\n",
    "        self.graph = graph\n",
    "        self.idx2node, self.node2idx = preprocess_nxgraph(graph)\n",
    "        self.use_alias = True\n",
    "\n",
    "        self.rep_size = embedding_size\n",
    "        self.order = order\n",
    "\n",
    "        self._embeddings = {}\n",
    "        self.negative_ratio = negative_ratio\n",
    "        self.order = order\n",
    "\n",
    "        self.node_size = graph.number_of_nodes()\n",
    "        self.edge_size = graph.number_of_edges()\n",
    "        self.samples_per_epoch = self.edge_size * (1 + negative_ratio)\n",
    "\n",
    "        self._gen_sampling_table()\n",
    "        self.reset_model()\n",
    "\n",
    "    def reset_training_config(self, batch_size, times):\n",
    "        self.batch_size = batch_size\n",
    "        self.steps_per_epoch = (\n",
    "                                       (self.samples_per_epoch - 1) // self.batch_size + 1) * times\n",
    "\n",
    "    def reset_model(self, opt='adam'):\n",
    "\n",
    "        self.model, self.embedding_dict = create_model(\n",
    "            self.node_size, self.rep_size, self.order)\n",
    "        self.model.compile(opt, line_loss)\n",
    "        self.batch_it = self.batch_iter(self.node2idx)\n",
    "\n",
    "    def _gen_sampling_table(self):\n",
    "\n",
    "        # create sampling table for vertex\n",
    "        power = 0.75\n",
    "        numNodes = self.node_size\n",
    "        node_degree = np.zeros(numNodes)  # out degree\n",
    "        node2idx = self.node2idx\n",
    "\n",
    "        for edge in self.graph.edges():\n",
    "            node_degree[node2idx[edge[0]]\n",
    "            ] += self.graph[edge[0]][edge[1]].get('weight', 1.0)\n",
    "\n",
    "        total_sum = sum([math.pow(node_degree[i], power)\n",
    "                         for i in range(numNodes)])\n",
    "        norm_prob = [float(math.pow(node_degree[j], power)) /\n",
    "                     total_sum for j in range(numNodes)]\n",
    "\n",
    "        self.node_accept, self.node_alias = create_alias_table(norm_prob)\n",
    "\n",
    "        # create sampling table for edge\n",
    "        numEdges = self.graph.number_of_edges()\n",
    "        total_sum = sum([self.graph[edge[0]][edge[1]].get('weight', 1.0)\n",
    "                         for edge in self.graph.edges()])\n",
    "        norm_prob = [self.graph[edge[0]][edge[1]].get('weight', 1.0) *\n",
    "                     numEdges / total_sum for edge in self.graph.edges()]\n",
    "\n",
    "        self.edge_accept, self.edge_alias = create_alias_table(norm_prob)\n",
    "\n",
    "    def batch_iter(self, node2idx):\n",
    "\n",
    "        edges = [(node2idx[x[0]], node2idx[x[1]]) for x in self.graph.edges()]\n",
    "\n",
    "        data_size = self.graph.number_of_edges()\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        # positive or negative mod\n",
    "        mod = 0\n",
    "        mod_size = 1 + self.negative_ratio\n",
    "        h = []\n",
    "        t = []\n",
    "        sign = 0\n",
    "        count = 0\n",
    "        start_index = 0\n",
    "        end_index = min(start_index + self.batch_size, data_size)\n",
    "        while True:\n",
    "            if mod == 0:\n",
    "\n",
    "                h = []\n",
    "                t = []\n",
    "                for i in range(start_index, end_index):\n",
    "                    if random.random() >= self.edge_accept[shuffle_indices[i]]:\n",
    "                        shuffle_indices[i] = self.edge_alias[shuffle_indices[i]]\n",
    "                    cur_h = edges[shuffle_indices[i]][0]\n",
    "                    cur_t = edges[shuffle_indices[i]][1]\n",
    "                    h.append(cur_h)\n",
    "                    t.append(cur_t)\n",
    "                sign = np.ones(len(h))\n",
    "            else:\n",
    "                sign = np.ones(len(h)) * -1\n",
    "                t = []\n",
    "                for i in range(len(h)):\n",
    "                    t.append(alias_sample(\n",
    "                        self.node_accept, self.node_alias))\n",
    "\n",
    "            if self.order == 'all':\n",
    "                yield ([np.array(h), np.array(t)], [sign, sign])\n",
    "            else:\n",
    "                yield ([np.array(h), np.array(t)], [sign])\n",
    "            mod += 1\n",
    "            mod %= mod_size\n",
    "            if mod == 0:\n",
    "                start_index = end_index\n",
    "                end_index = min(start_index + self.batch_size, data_size)\n",
    "\n",
    "            if start_index >= data_size:\n",
    "                count += 1\n",
    "                mod = 0\n",
    "                h = []\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                start_index = 0\n",
    "                end_index = min(start_index + self.batch_size, data_size)\n",
    "\n",
    "    def get_embeddings(self, ):\n",
    "        self._embeddings = {}\n",
    "        if self.order == 'first':\n",
    "            embeddings = self.embedding_dict['first'].get_weights()[0]\n",
    "        elif self.order == 'second':\n",
    "            embeddings = self.embedding_dict['second'].get_weights()[0]\n",
    "        else:\n",
    "            embeddings = np.hstack((self.embedding_dict['first'].get_weights()[\n",
    "                                        0], self.embedding_dict['second'].get_weights()[0]))\n",
    "        idx2node = self.idx2node\n",
    "        for i, embedding in enumerate(embeddings):\n",
    "            self._embeddings[idx2node[i]] = embedding\n",
    "\n",
    "        return self._embeddings\n",
    "\n",
    "    def train(self, batch_size=1024, epochs=1, initial_epoch=0, verbose=1, times=1):\n",
    "        self.reset_training_config(batch_size, times)\n",
    "        hist = self.model.fit_generator(self.batch_it, epochs=epochs, initial_epoch=initial_epoch,\n",
    "                                        steps_per_epoch=self.steps_per_epoch,\n",
    "                                        verbose=verbose)\n",
    "\n",
    "        return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1224/1224 - 4s - loss: 0.6871\n",
      "Epoch 2/50\n",
      "1224/1224 - 4s - loss: 0.5128\n",
      "Epoch 3/50\n",
      "1224/1224 - 4s - loss: 0.3438\n",
      "Epoch 4/50\n",
      "1224/1224 - 4s - loss: 0.2772\n",
      "Epoch 5/50\n",
      "1224/1224 - 4s - loss: 0.2377\n",
      "Epoch 6/50\n",
      "1224/1224 - 4s - loss: 0.2078\n",
      "Epoch 7/50\n",
      "1224/1224 - 4s - loss: 0.1835\n",
      "Epoch 8/50\n",
      "1224/1224 - 4s - loss: 0.1622\n",
      "Epoch 9/50\n",
      "1224/1224 - 4s - loss: 0.1407\n",
      "Epoch 10/50\n",
      "1224/1224 - 4s - loss: 0.1240\n",
      "Epoch 11/50\n",
      "1224/1224 - 4s - loss: 0.1064\n",
      "Epoch 12/50\n",
      "1224/1224 - 4s - loss: 0.0934\n",
      "Epoch 13/50\n",
      "1224/1224 - 4s - loss: 0.0796\n",
      "Epoch 14/50\n",
      "1224/1224 - 4s - loss: 0.0693\n",
      "Epoch 15/50\n",
      "1224/1224 - 4s - loss: 0.0608\n",
      "Epoch 16/50\n",
      "1224/1224 - 4s - loss: 0.0530\n",
      "Epoch 17/50\n",
      "1224/1224 - 4s - loss: 0.0465\n",
      "Epoch 18/50\n",
      "1224/1224 - 4s - loss: 0.0437\n",
      "Epoch 19/50\n",
      "1224/1224 - 4s - loss: 0.0409\n",
      "Epoch 20/50\n",
      "1224/1224 - 4s - loss: 0.0348\n",
      "Epoch 21/50\n",
      "1224/1224 - 4s - loss: 0.0329\n",
      "Epoch 22/50\n",
      "1224/1224 - 4s - loss: 0.0329\n",
      "Epoch 23/50\n",
      "1224/1224 - 4s - loss: 0.0313\n",
      "Epoch 24/50\n",
      "1224/1224 - 4s - loss: 0.0311\n",
      "Epoch 25/50\n",
      "1224/1224 - 4s - loss: 0.0290\n",
      "Epoch 26/50\n",
      "1224/1224 - 4s - loss: 0.0276\n",
      "Epoch 27/50\n",
      "1224/1224 - 4s - loss: 0.0269\n",
      "Epoch 28/50\n",
      "1224/1224 - 3s - loss: 0.0284\n",
      "Epoch 29/50\n",
      "1224/1224 - 4s - loss: 0.0261\n",
      "Epoch 30/50\n",
      "1224/1224 - 4s - loss: 0.0280\n",
      "Epoch 31/50\n",
      "1224/1224 - 4s - loss: 0.0256\n",
      "Epoch 32/50\n",
      "1224/1224 - 3s - loss: 0.0267\n",
      "Epoch 33/50\n",
      "1224/1224 - 3s - loss: 0.0249\n",
      "Epoch 34/50\n",
      "1224/1224 - 4s - loss: 0.0269\n",
      "Epoch 35/50\n",
      "1224/1224 - 4s - loss: 0.0251\n",
      "Epoch 36/50\n",
      "1224/1224 - 4s - loss: 0.0259\n",
      "Epoch 37/50\n",
      "1224/1224 - 4s - loss: 0.0248\n",
      "Epoch 38/50\n",
      "1224/1224 - 4s - loss: 0.0253\n",
      "Epoch 39/50\n",
      "1224/1224 - 3s - loss: 0.0262\n",
      "Epoch 40/50\n",
      "1224/1224 - 4s - loss: 0.0240\n",
      "Epoch 41/50\n",
      "1224/1224 - 4s - loss: 0.0254\n",
      "Epoch 42/50\n",
      "1224/1224 - 4s - loss: 0.0243\n",
      "Epoch 43/50\n",
      "1224/1224 - 4s - loss: 0.0252\n",
      "Epoch 44/50\n",
      "1224/1224 - 4s - loss: 0.0268\n",
      "Epoch 45/50\n",
      "1224/1224 - 4s - loss: 0.0257\n",
      "Epoch 46/50\n",
      "1224/1224 - 4s - loss: 0.0251\n",
      "Epoch 47/50\n",
      "1224/1224 - 4s - loss: 0.0236\n",
      "Epoch 48/50\n",
      "1224/1224 - 4s - loss: 0.0246\n",
      "Epoch 49/50\n",
      "1224/1224 - 4s - loss: 0.0245\n",
      "Epoch 50/50\n",
      "1224/1224 - 3s - loss: 0.0235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f38703593a0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LINE(G,embedding_size=50,order='second') #init model,order can be ['first','second','all']\n",
    "model.train(batch_size=64,epochs=50,verbose=2)# train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = model.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = embeddings_dict.keys()\n",
    "nodes_index=[]\n",
    "for  i in keys:\n",
    "    nodes_index.append(i)\n",
    "nodes_index = np.array(nodes_index)\n",
    "\n",
    "values = embeddings_dict.values()\n",
    "values_array = []\n",
    "for i in values:\n",
    "    values_array.append(i)\n",
    "values_array = np.array(values_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "embedding_filename = r\"/home/student/Vansh/GraphGAN/Pre-Train Embeddings/LINE_embeddings.emb\"\n",
    "index = np.array(nodes_index).reshape(-1, 1)\n",
    "embedding_matrix = np.hstack([index, values_array])\n",
    "embedding_list = embedding_matrix.tolist()\n",
    "embedding_str = [str(int(emb[0])) + \" \" + \" \".join([str(x) for x in emb[1:]]) + \"\\n\"\n",
    "                  for emb in embedding_list]\n",
    "with open(embedding_filename, \"w+\") as f:\n",
    "    lines = [str(5119) + \"\\t\" + str(50) + \"\\n\"] + embedding_str\n",
    "    f.writelines(lines)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d006cd09a7ebbb48eee4e86342fa19e01d16df56cb7967e71afee02ac2f94235"
  },
  "kernelspec": {
   "display_name": "Python 3.8.18 64-bit ('graphgan')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
