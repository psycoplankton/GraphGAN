{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"/home/student/Vansh/GraphGAN/CA-GrQc Dataset/CA-GrQc_train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NodeIDfrom</th>\n",
       "      <th>NodeIDto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4095</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3213</td>\n",
       "      <td>2059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1882</td>\n",
       "      <td>3269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4897</td>\n",
       "      <td>2661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2621</td>\n",
       "      <td>1718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13041</th>\n",
       "      <td>830</td>\n",
       "      <td>5103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13042</th>\n",
       "      <td>1842</td>\n",
       "      <td>3805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13043</th>\n",
       "      <td>3468</td>\n",
       "      <td>1758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13044</th>\n",
       "      <td>4889</td>\n",
       "      <td>3784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13045</th>\n",
       "      <td>2796</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13046 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NodeIDfrom  NodeIDto\n",
       "0            4095       546\n",
       "1            3213      2059\n",
       "2            1882      3269\n",
       "3            4897      2661\n",
       "4            2621      1718\n",
       "...           ...       ...\n",
       "13041         830      5103\n",
       "13042        1842      3805\n",
       "13043        3468      1758\n",
       "13044        4889      3784\n",
       "13045        2796       504\n",
       "\n",
       "[13046 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset,\n",
    "                sep = '\\t',\n",
    "                names = [\"NodeIDfrom\", \"NodeIDto\"],\n",
    "                )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5119"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the graph networkx object from the above dataframe\n",
    "\n",
    "G = nx.from_pandas_edgelist(df = df,\n",
    "                             source = \"NodeIDfrom\",\n",
    "                             target = \"NodeIDto\",\n",
    "                             create_using=nx.Graph())\n",
    "len(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_num(num, workers):\n",
    "    if num % workers == 0:\n",
    "        return [num // workers] * workers\n",
    "    else:\n",
    "        return [num // workers] * workers + [num % workers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_alias_table(area_ratio):\n",
    "    \"\"\"\n",
    "\n",
    "    :param area_ratio: sum(area_ratio)=1\n",
    "    :return: accept,alias\n",
    "    \"\"\"\n",
    "    l = len(area_ratio)\n",
    "    accept, alias = [0] * l, [0] * l\n",
    "    small, large = [], []\n",
    "    area_ratio_ = np.array(area_ratio) * l\n",
    "    for i, prob in enumerate(area_ratio_):\n",
    "        if prob < 1.0:\n",
    "            small.append(i)\n",
    "        else:\n",
    "            large.append(i)\n",
    "\n",
    "    while small and large:\n",
    "        small_idx, large_idx = small.pop(), large.pop()\n",
    "        accept[small_idx] = area_ratio_[small_idx]\n",
    "        alias[small_idx] = large_idx\n",
    "        area_ratio_[large_idx] = area_ratio_[large_idx] - \\\n",
    "                                 (1 - area_ratio_[small_idx])\n",
    "        if area_ratio_[large_idx] < 1.0:\n",
    "            small.append(large_idx)\n",
    "        else:\n",
    "            large.append(large_idx)\n",
    "\n",
    "    while large:\n",
    "        large_idx = large.pop()\n",
    "        accept[large_idx] = 1\n",
    "    while small:\n",
    "        small_idx = small.pop()\n",
    "        accept[small_idx] = 1\n",
    "\n",
    "    return accept, alias\n",
    "\n",
    "\n",
    "def alias_sample(accept, alias):\n",
    "    \"\"\"\n",
    "\n",
    "    :param accept:\n",
    "    :param alias:\n",
    "    :return: sample index\n",
    "    \"\"\"\n",
    "    N = len(accept)\n",
    "    i = int(np.random.random() * N)\n",
    "    r = np.random.random()\n",
    "    if r < accept[i]:\n",
    "        return i\n",
    "    else:\n",
    "        return alias[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "class RandomWalker:\n",
    "    def __init__(self, G, use_rejection_sampling=False):\n",
    "        \"\"\"\n",
    "        :param G: networkx Graph Object.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.G = G\n",
    "        self.use_rejection_sampling = use_rejection_sampling\n",
    "\n",
    "    def node2vec_walk(self, walk_length, start_node):\n",
    "\n",
    "        G = self.G\n",
    "        alias_nodes = self.alias_nodes\n",
    "        alias_edges = self.alias_edges\n",
    "\n",
    "        walk = [start_node]\n",
    "\n",
    "        while len(walk) < walk_length:\n",
    "            cur = walk[-1]\n",
    "            cur_nbrs = list(G.neighbors(cur))\n",
    "            if len(cur_nbrs) > 0:\n",
    "                if len(walk) == 1:\n",
    "                    walk.append(\n",
    "                        cur_nbrs[alias_sample(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
    "                else:\n",
    "                    prev = walk[-2]\n",
    "                    edge = (prev, cur)\n",
    "                    next_node = cur_nbrs[alias_sample(alias_edges[edge][0],\n",
    "                                                      alias_edges[edge][1])]\n",
    "                    walk.append(next_node)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return walk\n",
    "    \n",
    "    def simulate_walks(self, num_walks, walk_length, workers=1, verbose=0):\n",
    "\n",
    "        G = self.G\n",
    "\n",
    "        nodes = list(G.nodes())\n",
    "\n",
    "        results = Parallel(n_jobs=workers, verbose=verbose, )(\n",
    "            delayed(self._simulate_walks)(nodes, num, walk_length) for num in\n",
    "            partition_num(num_walks, workers))\n",
    "\n",
    "        walks = list(itertools.chain(*results))\n",
    "\n",
    "        return walks\n",
    "\n",
    "    def _simulate_walks(self, nodes, num_walks, walk_length, ):\n",
    "        walks = []\n",
    "        for _ in range(num_walks):\n",
    "            random.shuffle(nodes)\n",
    "            for v in nodes:\n",
    "                walks.append(self.node2vec_walk(\n",
    "                    walk_length=walk_length, start_node=v))\n",
    "        return walks\n",
    "    \n",
    "    def preprocess_transition_probs(self):\n",
    "        \"\"\"\n",
    "        Preprocessing of transition probabilities for guiding the random walks.\n",
    "        \"\"\"\n",
    "        G = self.G\n",
    "        alias_nodes = {}\n",
    "        for node in G.nodes():\n",
    "            unnormalized_probs = [G[node][nbr].get('weight', 1.0)\n",
    "                                  for nbr in G.neighbors(node)]\n",
    "            norm_const = sum(unnormalized_probs)\n",
    "            normalized_probs = [\n",
    "                float(u_prob) / norm_const for u_prob in unnormalized_probs]\n",
    "            alias_nodes[node] = create_alias_table(normalized_probs)\n",
    "\n",
    "        if not self.use_rejection_sampling:\n",
    "            alias_edges = {}\n",
    "\n",
    "            for edge in G.edges():\n",
    "                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "                if not G.is_directed():\n",
    "                    alias_edges[(edge[1], edge[0])] = self.get_alias_edge(edge[1], edge[0])\n",
    "                self.alias_edges = alias_edges\n",
    "\n",
    "        self.alias_nodes = alias_nodes\n",
    "        return\n",
    "    \n",
    "    def get_alias_edge(self, t, v):\n",
    "        \"\"\"\n",
    "        compute unnormalized transition probability between nodes v and its neighbors give the previous visited node t.\n",
    "        :param t:\n",
    "        :param v:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        G = self.G\n",
    "        p = 1\n",
    "        q = 1\n",
    "\n",
    "        unnormalized_probs = []\n",
    "        for x in G.neighbors(v):\n",
    "            weight = G[v][x].get('weight', 1.0)  # w_vx\n",
    "            if x == t:  # d_tx == 0\n",
    "                unnormalized_probs.append(weight / p)\n",
    "            elif G.has_edge(x, t):  # d_tx == 1\n",
    "                unnormalized_probs.append(weight)\n",
    "            else:  # d_tx > 1\n",
    "                unnormalized_probs.append(weight / q)\n",
    "        norm_const = sum(unnormalized_probs)\n",
    "        normalized_probs = [\n",
    "            float(u_prob) / norm_const for u_prob in unnormalized_probs]\n",
    "\n",
    "        return create_alias_table(normalized_probs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "class Node2Vec:\n",
    "\n",
    "    def __init__(self, graph, walk_length, num_walks, workers=1, use_rejection_sampling = False):\n",
    "\n",
    "        self.graph = graph\n",
    "        self.walker = RandomWalker(G)\n",
    "\n",
    "        print(\"Preprocess transition probs...\")\n",
    "        self.walker.preprocess_transition_probs()\n",
    "\n",
    "        self.sentences = self.walker.simulate_walks(\n",
    "            num_walks=num_walks, walk_length=walk_length, workers=workers, verbose=1)\n",
    "\n",
    "    def train(self, embed_size=128, window_size=5, workers=3, iter=5, **kwargs):\n",
    "        kwargs[\"sentences\"] = self.sentences\n",
    "        kwargs[\"min_count\"] = kwargs.get(\"min_count\", 0)\n",
    "        kwargs[\"vector_size\"] = embed_size\n",
    "        kwargs[\"sg\"] = 1\n",
    "        kwargs[\"hs\"] = 0  # node2vec not use Hierarchical Softmax\n",
    "        kwargs[\"workers\"] = workers\n",
    "        kwargs[\"window\"] = window_size\n",
    "        kwargs[\"epochs\"] = iter\n",
    "\n",
    "        print(\"Learning embedding vectors...\")\n",
    "        model = Word2Vec(**kwargs)\n",
    "        print(\"Learning embedding vectors done!\")\n",
    "\n",
    "        self.w2v_model = model\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess transition probs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning embedding vectors...\n",
      "Learning embedding vectors done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7fc03284a640>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Node2Vec(graph = G,\n",
    "                 walk_length = 10,\n",
    "                 num_walks = 5)\n",
    "model.train(embed_size=50,\n",
    "            iter = 3,\n",
    "            window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"/home/student/Vansh/GraphGAN/Pre-Train Embeddings/Node2Vec_embeddings.emb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.17846364e-01, -1.25962608e-02, -3.58589590e-01,  4.16712798e-02,\n",
       "        -2.49028563e-01,  9.13286302e-03,  2.23333016e-02,  4.13231492e-01,\n",
       "        -2.28128865e-01, -2.11545512e-01, -4.13643926e-01, -1.16660759e-01,\n",
       "         2.04055503e-01,  3.35868567e-01,  8.33163783e-03, -1.60395890e-01,\n",
       "        -1.58585981e-02, -2.97225267e-01, -4.99293879e-02, -2.32827812e-01,\n",
       "         5.67359067e-02,  4.28136438e-01,  2.69486308e-01, -2.69341201e-01,\n",
       "         1.10049084e-01, -1.27384111e-01,  2.76594702e-02, -4.00493145e-01,\n",
       "        -3.80183846e-01, -9.01873503e-03, -1.66814774e-01,  8.42399150e-02,\n",
       "        -3.85455161e-01, -2.30530307e-01,  1.93758026e-01,  3.44469965e-01,\n",
       "        -3.02146334e-04,  9.25938636e-02,  1.07589208e-01, -2.98619449e-01,\n",
       "         4.66027111e-02, -2.81947970e-01, -6.04567043e-02,  3.40041786e-01,\n",
       "         4.36361969e-01,  1.36416286e-01,  9.90648717e-02, -4.82427984e-01,\n",
       "         6.04352392e-02,  5.23666143e-01], dtype=float32),\n",
       " 2.0936053)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = []\n",
    "for i in G.nodes():\n",
    "    embeddings.append(model.w2v_model.wv[i]) #/np.linalg.norm(model.wv[i]))\n",
    "embeddings = np.array(embeddings)\n",
    "embeddings[0], np.max(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "embedding_filename = r\"/home/student/Vansh/GraphGAN/Pre-Train Embeddings/Node2Vec_embeddings.emb\"\n",
    "index = np.array(G.nodes()).reshape(-1, 1)\n",
    "embedding_matrix = np.hstack([index, embeddings])\n",
    "embedding_list = embedding_matrix.tolist()\n",
    "embedding_str = [str(int(emb[0])) + \" \" + \" \".join([str(x) for x in emb[1:]]) + \"\\n\"\n",
    "                  for emb in embedding_list]\n",
    "with open(embedding_filename, \"w+\") as f:\n",
    "    lines = [str(5119) + \"\\t\" + str(50) + \"\\n\"] + embedding_str\n",
    "    f.writelines(lines)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d006cd09a7ebbb48eee4e86342fa19e01d16df56cb7967e71afee02ac2f94235"
  },
  "kernelspec": {
   "display_name": "Python 3.8.18 64-bit ('graphgan')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
