{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"/home/student/Vansh/GraphGAN/CA-GrQc Dataset/CA-GrQc_train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NodeIDfrom</th>\n",
       "      <th>NodeIDto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4095</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3213</td>\n",
       "      <td>2059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1882</td>\n",
       "      <td>3269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4897</td>\n",
       "      <td>2661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2621</td>\n",
       "      <td>1718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13041</th>\n",
       "      <td>830</td>\n",
       "      <td>5103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13042</th>\n",
       "      <td>1842</td>\n",
       "      <td>3805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13043</th>\n",
       "      <td>3468</td>\n",
       "      <td>1758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13044</th>\n",
       "      <td>4889</td>\n",
       "      <td>3784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13045</th>\n",
       "      <td>2796</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13046 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NodeIDfrom  NodeIDto\n",
       "0            4095       546\n",
       "1            3213      2059\n",
       "2            1882      3269\n",
       "3            4897      2661\n",
       "4            2621      1718\n",
       "...           ...       ...\n",
       "13041         830      5103\n",
       "13042        1842      3805\n",
       "13043        3468      1758\n",
       "13044        4889      3784\n",
       "13045        2796       504\n",
       "\n",
       "[13046 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset,\n",
    "                sep = '\\t',\n",
    "                names = [\"NodeIDfrom\", \"NodeIDto\"],\n",
    "                )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5119"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the graph networkx object from the above dataframe\n",
    "\n",
    "G = nx.from_pandas_edgelist(df = df,\n",
    "                             source = \"NodeIDfrom\",\n",
    "                             target = \"NodeIDto\",\n",
    "                             create_using=nx.Graph())\n",
    "len(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_alias_table(area_ratio):\n",
    "    \"\"\"\n",
    "\n",
    "    :param area_ratio: sum(area_ratio)=1\n",
    "    :return: accept,alias\n",
    "    \"\"\"\n",
    "    l = len(area_ratio)\n",
    "    accept, alias = [0] * l, [0] * l\n",
    "    small, large = [], []\n",
    "    area_ratio_ = np.array(area_ratio) * l\n",
    "    for i, prob in enumerate(area_ratio_):\n",
    "        if prob < 1.0:\n",
    "            small.append(i)\n",
    "        else:\n",
    "            large.append(i)\n",
    "\n",
    "    while small and large:\n",
    "        small_idx, large_idx = small.pop(), large.pop()\n",
    "        accept[small_idx] = area_ratio_[small_idx]\n",
    "        alias[small_idx] = large_idx\n",
    "        area_ratio_[large_idx] = area_ratio_[large_idx] - \\\n",
    "                                 (1 - area_ratio_[small_idx])\n",
    "        if area_ratio_[large_idx] < 1.0:\n",
    "            small.append(large_idx)\n",
    "        else:\n",
    "            large.append(large_idx)\n",
    "\n",
    "    while large:\n",
    "        large_idx = large.pop()\n",
    "        accept[large_idx] = 1\n",
    "    while small:\n",
    "        small_idx = small.pop()\n",
    "        accept[small_idx] = 1\n",
    "\n",
    "    return accept, alias\n",
    "\n",
    "\n",
    "def alias_sample(accept, alias):\n",
    "    \"\"\"\n",
    "\n",
    "    :param accept:\n",
    "    :param alias:\n",
    "    :return: sample index\n",
    "    \"\"\"\n",
    "    N = len(accept)\n",
    "    i = int(np.random.random() * N)\n",
    "    r = np.random.random()\n",
    "    if r < accept[i]:\n",
    "        return i\n",
    "    else:\n",
    "        return alias[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nxgraph(graph):\n",
    "    node2idx = {}\n",
    "    idx2node = []\n",
    "    node_size = 0\n",
    "    for node in graph.nodes():\n",
    "        node2idx[node] = node_size\n",
    "        idx2node.append(node)\n",
    "        node_size += 1\n",
    "    return idx2node, node2idx\n",
    "\n",
    "\n",
    "def partition_dict(vertices, workers):\n",
    "    batch_size = (len(vertices) - 1) // workers + 1\n",
    "    part_list = []\n",
    "    part = []\n",
    "    count = 0\n",
    "    for v1, nbs in vertices.items():\n",
    "        part.append((v1, nbs))\n",
    "        count += 1\n",
    "        if count % batch_size == 0:\n",
    "            part_list.append(part)\n",
    "            part = []\n",
    "    if len(part) > 0:\n",
    "        part_list.append(part)\n",
    "    return part_list\n",
    "\n",
    "def partition_num(num, workers):\n",
    "    if num % workers == 0:\n",
    "        return [num // workers] * workers\n",
    "    else:\n",
    "        return [num // workers] * workers + [num % workers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from joblib import Parallel, delayed\n",
    "class BiasedWalker:\n",
    "    def __init__(self, idx2node, temp_path):\n",
    "\n",
    "        self.idx2node = idx2node\n",
    "        self.idx = list(range(len(self.idx2node)))\n",
    "        self.temp_path = temp_path\n",
    "        pass\n",
    "\n",
    "    def simulate_walks(self, num_walks, walk_length, stay_prob=0.3, workers=1, verbose=0):\n",
    "\n",
    "        layers_adj = pd.read_pickle(self.temp_path + 'layers_adj.pkl')\n",
    "        layers_alias = pd.read_pickle(self.temp_path + 'layers_alias.pkl')\n",
    "        layers_accept = pd.read_pickle(self.temp_path + 'layers_accept.pkl')\n",
    "        gamma = pd.read_pickle(self.temp_path + 'gamma.pkl')\n",
    "\n",
    "        nodes = self.idx  # list(self.g.nodes())\n",
    "\n",
    "        results = Parallel(n_jobs=workers, verbose=verbose, )(\n",
    "            delayed(self._simulate_walks)(nodes, num, walk_length, stay_prob, layers_adj, layers_accept, layers_alias,\n",
    "                                          gamma) for num in\n",
    "            partition_num(num_walks, workers))\n",
    "\n",
    "        walks = list(itertools.chain(*results))\n",
    "        return walks\n",
    "\n",
    "    def _simulate_walks(self, nodes, num_walks, walk_length, stay_prob, layers_adj, layers_accept, layers_alias, gamma):\n",
    "        walks = []\n",
    "        for _ in range(num_walks):\n",
    "            random.shuffle(nodes)\n",
    "            for v in nodes:\n",
    "                walks.append(self._exec_random_walk(layers_adj, layers_accept, layers_alias,\n",
    "                                                    v, walk_length, gamma, stay_prob))\n",
    "        return walks\n",
    "\n",
    "    def _exec_random_walk(self, graphs, layers_accept, layers_alias, v, walk_length, gamma, stay_prob=0.3):\n",
    "        initialLayer = 0\n",
    "        layer = initialLayer\n",
    "\n",
    "        path = []\n",
    "        path.append(self.idx2node[v])\n",
    "\n",
    "        while len(path) < walk_length:\n",
    "            r = random.random()\n",
    "            if (r < stay_prob):  # same layer\n",
    "                v = chooseNeighbor(v, graphs, layers_alias,\n",
    "                                   layers_accept, layer)\n",
    "                path.append(self.idx2node[v])\n",
    "            else:  # different layer\n",
    "                r = random.random()\n",
    "                try:\n",
    "                    x = math.log(gamma[layer][v] + math.e)\n",
    "                    p_moveup = (x / (x + 1))\n",
    "                except:\n",
    "                    print(layer, v)\n",
    "                    raise ValueError()\n",
    "\n",
    "                if (r > p_moveup):\n",
    "                    if (layer > initialLayer):\n",
    "                        layer = layer - 1\n",
    "                else:\n",
    "                    if ((layer + 1) in graphs and v in graphs[layer + 1]):\n",
    "                        layer = layer + 1\n",
    "\n",
    "        return path\n",
    "\n",
    "\n",
    "def chooseNeighbor(v, graphs, layers_alias, layers_accept, layer):\n",
    "    v_list = graphs[layer][v]\n",
    "\n",
    "    idx = alias_sample(layers_accept[layer][v], layers_alias[layer][v])\n",
    "    v = v_list[idx]\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import shutil\n",
    "from collections import ChainMap, deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastdtw import fastdtw\n",
    "from gensim.models import Word2Vec\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Struc2Vec():\n",
    "    def __init__(self, graph, walk_length=10, num_walks=100, workers=1, verbose=0, stay_prob=0.3, opt1_reduce_len=True,\n",
    "                 opt2_reduce_sim_calc=True, opt3_num_layers=None, temp_path='./temp_struc2vec/', reuse=False):\n",
    "        self.graph = graph\n",
    "        self.idx2node, self.node2idx = preprocess_nxgraph(graph)\n",
    "        self.idx = list(range(len(self.idx2node)))\n",
    "\n",
    "        self.opt1_reduce_len = opt1_reduce_len\n",
    "        self.opt2_reduce_sim_calc = opt2_reduce_sim_calc\n",
    "        self.opt3_num_layers = opt3_num_layers\n",
    "\n",
    "        self.resue = reuse\n",
    "        self.temp_path = temp_path\n",
    "\n",
    "        if not os.path.exists(self.temp_path):\n",
    "            os.mkdir(self.temp_path)\n",
    "        if not reuse:\n",
    "            shutil.rmtree(self.temp_path)\n",
    "            os.mkdir(self.temp_path)\n",
    "\n",
    "        self.create_context_graph(self.opt3_num_layers, workers, verbose)\n",
    "        self.prepare_biased_walk()\n",
    "        self.walker = BiasedWalker(self.idx2node, self.temp_path)\n",
    "        self.sentences = self.walker.simulate_walks(\n",
    "            num_walks, walk_length, stay_prob, workers, verbose)\n",
    "\n",
    "        self._embeddings = {}\n",
    "\n",
    "    def create_context_graph(self, max_num_layers, workers=1, verbose=0, ):\n",
    "\n",
    "        pair_distances = self._compute_structural_distance(\n",
    "            max_num_layers, workers, verbose, )\n",
    "        layers_adj, layers_distances = self._get_layer_rep(pair_distances)\n",
    "        pd.to_pickle(layers_adj, self.temp_path + 'layers_adj.pkl')\n",
    "\n",
    "        layers_accept, layers_alias = self._get_transition_probs(\n",
    "            layers_adj, layers_distances)\n",
    "        pd.to_pickle(layers_alias, self.temp_path + 'layers_alias.pkl')\n",
    "        pd.to_pickle(layers_accept, self.temp_path + 'layers_accept.pkl')\n",
    "\n",
    "    def prepare_biased_walk(self, ):\n",
    "\n",
    "        sum_weights = {}\n",
    "        sum_edges = {}\n",
    "        average_weight = {}\n",
    "        gamma = {}\n",
    "        layer = 0\n",
    "        while (os.path.exists(self.temp_path + 'norm_weights_distance-layer-' + str(layer) + '.pkl')):\n",
    "            probs = pd.read_pickle(\n",
    "                self.temp_path + 'norm_weights_distance-layer-' + str(layer) + '.pkl')\n",
    "            for v, list_weights in probs.items():\n",
    "                sum_weights.setdefault(layer, 0)\n",
    "                sum_edges.setdefault(layer, 0)\n",
    "                sum_weights[layer] += sum(list_weights)\n",
    "                sum_edges[layer] += len(list_weights)\n",
    "\n",
    "            average_weight[layer] = sum_weights[layer] / sum_edges[layer]\n",
    "\n",
    "            gamma.setdefault(layer, {})\n",
    "\n",
    "            for v, list_weights in probs.items():\n",
    "                num_neighbours = 0\n",
    "                for w in list_weights:\n",
    "                    if (w > average_weight[layer]):\n",
    "                        num_neighbours += 1\n",
    "                gamma[layer][v] = num_neighbours\n",
    "\n",
    "            layer += 1\n",
    "\n",
    "        pd.to_pickle(average_weight, self.temp_path + 'average_weight')\n",
    "        pd.to_pickle(gamma, self.temp_path + 'gamma.pkl')\n",
    "\n",
    "    def train(self, embed_size=128, window_size=5, workers=3, iter=5):\n",
    "\n",
    "        # pd.read_pickle(self.temp_path+'walks.pkl')\n",
    "        sentences = self.sentences\n",
    "\n",
    "        print(\"Learning representation...\")\n",
    "        model = Word2Vec(sentences, vector_size=embed_size, window=window_size, min_count=0, hs=1, sg=1,\n",
    "                         workers=workers,\n",
    "                         epochs=iter)\n",
    "        print(\"Learning representation done!\")\n",
    "        self.w2v_model = model\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_embeddings(self, ):\n",
    "        if self.w2v_model is None:\n",
    "            print(\"model not train\")\n",
    "            return {}\n",
    "\n",
    "        self._embeddings = {}\n",
    "        for word in self.graph.nodes():\n",
    "            self._embeddings[word] = self.w2v_model.wv[word]\n",
    "\n",
    "        return self._embeddings\n",
    "\n",
    "    def _compute_ordered_degreelist(self, max_num_layers):\n",
    "\n",
    "        degreeList = {}\n",
    "        vertices = self.idx  # self.g.nodes()\n",
    "        for v in vertices:\n",
    "            degreeList[v] = self._get_order_degreelist_node(v, max_num_layers)\n",
    "        return degreeList\n",
    "\n",
    "    def _get_order_degreelist_node(self, root, max_num_layers=None):\n",
    "        if max_num_layers is None:\n",
    "            max_num_layers = float('inf')\n",
    "\n",
    "        ordered_degree_sequence_dict = {}\n",
    "        visited = [False] * len(self.graph.nodes())\n",
    "        queue = deque()\n",
    "        level = 0\n",
    "        queue.append(root)\n",
    "        visited[root] = True\n",
    "\n",
    "        while (len(queue) > 0 and level <= max_num_layers):\n",
    "\n",
    "            count = len(queue)\n",
    "            if self.opt1_reduce_len:\n",
    "                degree_list = {}\n",
    "            else:\n",
    "                degree_list = []\n",
    "            while (count > 0):\n",
    "\n",
    "                top = queue.popleft()\n",
    "                node = self.idx2node[top]\n",
    "                degree = len(self.graph[node])\n",
    "\n",
    "                if self.opt1_reduce_len:\n",
    "                    degree_list[degree] = degree_list.get(degree, 0) + 1\n",
    "                else:\n",
    "                    degree_list.append(degree)\n",
    "\n",
    "                for nei in self.graph[node]:\n",
    "                    nei_idx = self.node2idx[nei]\n",
    "                    if not visited[nei_idx]:\n",
    "                        visited[nei_idx] = True\n",
    "                        queue.append(nei_idx)\n",
    "                count -= 1\n",
    "            if self.opt1_reduce_len:\n",
    "                orderd_degree_list = [(degree, freq)\n",
    "                                      for degree, freq in degree_list.items()]\n",
    "                orderd_degree_list.sort(key=lambda x: x[0])\n",
    "            else:\n",
    "                orderd_degree_list = sorted(degree_list)\n",
    "            ordered_degree_sequence_dict[level] = orderd_degree_list\n",
    "            level += 1\n",
    "\n",
    "        return ordered_degree_sequence_dict\n",
    "\n",
    "    def _compute_structural_distance(self, max_num_layers, workers=1, verbose=0, ):\n",
    "\n",
    "        if os.path.exists(self.temp_path + 'structural_dist.pkl'):\n",
    "            structural_dist = pd.read_pickle(\n",
    "                self.temp_path + 'structural_dist.pkl')\n",
    "        else:\n",
    "            if self.opt1_reduce_len:\n",
    "                dist_func = cost_max\n",
    "            else:\n",
    "                dist_func = cost\n",
    "\n",
    "            if os.path.exists(self.temp_path + 'degreelist.pkl'):\n",
    "                degreeList = pd.read_pickle(self.temp_path + 'degreelist.pkl')\n",
    "            else:\n",
    "                degreeList = self._compute_ordered_degreelist(max_num_layers)\n",
    "                pd.to_pickle(degreeList, self.temp_path + 'degreelist.pkl')\n",
    "\n",
    "            if self.opt2_reduce_sim_calc:\n",
    "                degrees = self._create_vectors()\n",
    "                degreeListsSelected = {}\n",
    "                vertices = {}\n",
    "                n_nodes = len(self.idx)\n",
    "                for v in self.idx:  # c:list of vertex\n",
    "                    nbs = get_vertices(\n",
    "                        v, len(self.graph[self.idx2node[v]]), degrees, n_nodes)\n",
    "                    vertices[v] = nbs  # store nbs\n",
    "                    degreeListsSelected[v] = degreeList[v]  # store dist\n",
    "                    for n in nbs:\n",
    "                        # store dist of nbs\n",
    "                        degreeListsSelected[n] = degreeList[n]\n",
    "            else:\n",
    "                vertices = {}\n",
    "                for v in degreeList:\n",
    "                    vertices[v] = [vd for vd in degreeList.keys() if vd > v]\n",
    "\n",
    "            results = Parallel(n_jobs=workers, verbose=verbose, )(\n",
    "                delayed(compute_dtw_dist)(part_list, degreeList, dist_func) for part_list in\n",
    "                partition_dict(vertices, workers))\n",
    "            dtw_dist = dict(ChainMap(*results))\n",
    "\n",
    "            structural_dist = convert_dtw_struc_dist(dtw_dist)\n",
    "            pd.to_pickle(structural_dist, self.temp_path +\n",
    "                         'structural_dist.pkl')\n",
    "\n",
    "        return structural_dist\n",
    "\n",
    "    def _create_vectors(self):\n",
    "        degrees = {}  # sotre v list of degree\n",
    "        degrees_sorted = set()  # store degree\n",
    "        G = self.graph\n",
    "        for v in self.idx:\n",
    "            degree = len(G[self.idx2node[v]])\n",
    "            degrees_sorted.add(degree)\n",
    "            if (degree not in degrees):\n",
    "                degrees[degree] = {}\n",
    "                degrees[degree]['vertices'] = []\n",
    "            degrees[degree]['vertices'].append(v)\n",
    "        degrees_sorted = np.array(list(degrees_sorted), dtype='int')\n",
    "        degrees_sorted = np.sort(degrees_sorted)\n",
    "\n",
    "        l = len(degrees_sorted)\n",
    "        for index, degree in enumerate(degrees_sorted):\n",
    "            if (index > 0):\n",
    "                degrees[degree]['before'] = degrees_sorted[index - 1]\n",
    "            if (index < (l - 1)):\n",
    "                degrees[degree]['after'] = degrees_sorted[index + 1]\n",
    "\n",
    "        return degrees\n",
    "\n",
    "    def _get_layer_rep(self, pair_distances):\n",
    "        layer_distances = {}\n",
    "        layer_adj = {}\n",
    "        for v_pair, layer_dist in pair_distances.items():\n",
    "            for layer, distance in layer_dist.items():\n",
    "                vx = v_pair[0]\n",
    "                vy = v_pair[1]\n",
    "\n",
    "                layer_distances.setdefault(layer, {})\n",
    "                layer_distances[layer][vx, vy] = distance\n",
    "\n",
    "                layer_adj.setdefault(layer, {})\n",
    "                layer_adj[layer].setdefault(vx, [])\n",
    "                layer_adj[layer].setdefault(vy, [])\n",
    "                layer_adj[layer][vx].append(vy)\n",
    "                layer_adj[layer][vy].append(vx)\n",
    "\n",
    "        return layer_adj, layer_distances\n",
    "\n",
    "    def _get_transition_probs(self, layers_adj, layers_distances):\n",
    "        layers_alias = {}\n",
    "        layers_accept = {}\n",
    "\n",
    "        for layer in layers_adj:\n",
    "\n",
    "            neighbors = layers_adj[layer]\n",
    "            layer_distances = layers_distances[layer]\n",
    "            node_alias_dict = {}\n",
    "            node_accept_dict = {}\n",
    "            norm_weights = {}\n",
    "\n",
    "            for v, neighbors in neighbors.items():\n",
    "                e_list = []\n",
    "                sum_w = 0.0\n",
    "\n",
    "                for n in neighbors:\n",
    "                    if (v, n) in layer_distances:\n",
    "                        wd = layer_distances[v, n]\n",
    "                    else:\n",
    "                        wd = layer_distances[n, v]\n",
    "                    w = np.exp(-float(wd))\n",
    "                    e_list.append(w)\n",
    "                    sum_w += w\n",
    "\n",
    "                e_list = [x / sum_w for x in e_list]\n",
    "                norm_weights[v] = e_list\n",
    "                accept, alias = create_alias_table(e_list)\n",
    "                node_alias_dict[v] = alias\n",
    "                node_accept_dict[v] = accept\n",
    "\n",
    "            pd.to_pickle(\n",
    "                norm_weights, self.temp_path + 'norm_weights_distance-layer-' + str(layer) + '.pkl')\n",
    "\n",
    "            layers_alias[layer] = node_alias_dict\n",
    "            layers_accept[layer] = node_accept_dict\n",
    "\n",
    "        return layers_accept, layers_alias\n",
    "\n",
    "\n",
    "def cost(a, b):\n",
    "    ep = 0.5\n",
    "    m = max(a, b) + ep\n",
    "    mi = min(a, b) + ep\n",
    "    return ((m / mi) - 1)\n",
    "\n",
    "\n",
    "def cost_min(a, b):\n",
    "    ep = 0.5\n",
    "    m = max(a[0], b[0]) + ep\n",
    "    mi = min(a[0], b[0]) + ep\n",
    "    return ((m / mi) - 1) * min(a[1], b[1])\n",
    "\n",
    "\n",
    "def cost_max(a, b):\n",
    "    ep = 0.5\n",
    "    m = max(a[0], b[0]) + ep\n",
    "    mi = min(a[0], b[0]) + ep\n",
    "    return ((m / mi) - 1) * max(a[1], b[1])\n",
    "\n",
    "\n",
    "def convert_dtw_struc_dist(distances, startLayer=1):\n",
    "    \"\"\"\n",
    "\n",
    "    :param distances: dict of dict\n",
    "    :param startLayer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for vertices, layers in distances.items():\n",
    "        keys_layers = sorted(layers.keys())\n",
    "        startLayer = min(len(keys_layers), startLayer)\n",
    "        for layer in range(0, startLayer):\n",
    "            keys_layers.pop(0)\n",
    "\n",
    "        for layer in keys_layers:\n",
    "            layers[layer] += layers[layer - 1]\n",
    "    return distances\n",
    "\n",
    "\n",
    "def get_vertices(v, degree_v, degrees, n_nodes):\n",
    "    a_vertices_selected = 2 * math.log(n_nodes, 2)\n",
    "    vertices = []\n",
    "    try:\n",
    "        c_v = 0\n",
    "\n",
    "        for v2 in degrees[degree_v]['vertices']:\n",
    "            if (v != v2):\n",
    "                vertices.append(v2)  # same degree\n",
    "                c_v += 1\n",
    "                if (c_v > a_vertices_selected):\n",
    "                    raise StopIteration\n",
    "\n",
    "        if ('before' not in degrees[degree_v]):\n",
    "            degree_b = -1\n",
    "        else:\n",
    "            degree_b = degrees[degree_v]['before']\n",
    "        if ('after' not in degrees[degree_v]):\n",
    "            degree_a = -1\n",
    "        else:\n",
    "            degree_a = degrees[degree_v]['after']\n",
    "        if (degree_b == -1 and degree_a == -1):\n",
    "            raise StopIteration  # not anymore v\n",
    "        degree_now = verifyDegrees(degrees, degree_v, degree_a, degree_b)\n",
    "        # nearest valid degree\n",
    "        while True:\n",
    "            for v2 in degrees[degree_now]['vertices']:\n",
    "                if (v != v2):\n",
    "                    vertices.append(v2)\n",
    "                    c_v += 1\n",
    "                    if (c_v > a_vertices_selected):\n",
    "                        raise StopIteration\n",
    "\n",
    "            if (degree_now == degree_b):\n",
    "                if ('before' not in degrees[degree_b]):\n",
    "                    degree_b = -1\n",
    "                else:\n",
    "                    degree_b = degrees[degree_b]['before']\n",
    "            else:\n",
    "                if ('after' not in degrees[degree_a]):\n",
    "                    degree_a = -1\n",
    "                else:\n",
    "                    degree_a = degrees[degree_a]['after']\n",
    "\n",
    "            if (degree_b == -1 and degree_a == -1):\n",
    "                raise StopIteration\n",
    "\n",
    "            degree_now = verifyDegrees(degrees, degree_v, degree_a, degree_b)\n",
    "\n",
    "    except StopIteration:\n",
    "        return list(vertices)\n",
    "\n",
    "    return list(vertices)\n",
    "\n",
    "\n",
    "def verifyDegrees(degrees, degree_v_root, degree_a, degree_b):\n",
    "    if (degree_b == -1):\n",
    "        degree_now = degree_a\n",
    "    elif (degree_a == -1):\n",
    "        degree_now = degree_b\n",
    "    elif (abs(degree_b - degree_v_root) < abs(degree_a - degree_v_root)):\n",
    "        degree_now = degree_b\n",
    "    else:\n",
    "        degree_now = degree_a\n",
    "\n",
    "    return degree_now\n",
    "\n",
    "\n",
    "def compute_dtw_dist(part_list, degreeList, dist_func):\n",
    "    dtw_dist = {}\n",
    "    for v1, nbs in part_list:\n",
    "        lists_v1 = degreeList[v1]  # lists_v1 :orderd degree list of v1\n",
    "        for v2 in nbs:\n",
    "            lists_v2 = degreeList[v2]  # lists_v1 :orderd degree list of v2\n",
    "            max_layer = min(len(lists_v1), len(lists_v2))  # valid layer\n",
    "            dtw_dist[v1, v2] = {}\n",
    "            for layer in range(0, max_layer):\n",
    "                dist, path = fastdtw(\n",
    "                    lists_v1[layer], lists_v2[layer], radius=1, dist=dist_func)\n",
    "                dtw_dist[v1, v2][layer] = dist\n",
    "    return dtw_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:  2.0min remaining:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  2.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  2.8min finished\n",
      "/tmp/ipykernel_8163/2732435472.py:279: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  e_list = [x / sum_w for x in e_list]\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   21.1s remaining:   21.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   35.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   35.0s finished\n"
     ]
    }
   ],
   "source": [
    "struc2vec = Struc2Vec(G, \n",
    "                      num_walks= 80, \n",
    "                      walk_length=10,\n",
    "                      workers=4, \n",
    "                      verbose=40) #init model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(struc2vec.sentences, epochs = 10, vector_size=50, window=5, hs=0, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.15020575,  0.2847751 ,  0.21364321,  0.12483591, -0.42199698,\n",
       "         0.02809624,  0.4358126 ,  0.4810618 ,  0.33756807,  0.11111433,\n",
       "        -0.6513837 , -0.5650549 ,  0.8091143 ,  0.7252025 , -0.48266056,\n",
       "        -0.28282762,  0.6679982 ,  0.31310752, -0.87903535,  0.0037066 ,\n",
       "         1.0581367 ,  0.14752766,  1.2324266 , -0.19181502,  1.2100717 ,\n",
       "        -0.08693918, -0.4318972 ,  0.9439232 , -1.0764228 , -0.45830128,\n",
       "         0.2855908 , -0.06172033, -0.35336584,  0.25700113,  0.5082788 ,\n",
       "         0.08436672,  0.4697838 ,  0.8811719 ,  0.20629226, -0.85342383,\n",
       "        -0.12083749,  0.38708198, -0.51941264,  0.07228836,  1.1358525 ,\n",
       "         0.5110627 , -0.4980323 , -1.0682466 ,  0.21530649,  0.20558591],\n",
       "       dtype=float32),\n",
       " 2.5589461)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = []\n",
    "for i in G.nodes():\n",
    "    embeddings.append(model.wv[i]) \n",
    "embeddings = np.array(embeddings)\n",
    "embeddings[0], np.max(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "embedding_filename = r\"/home/student/Vansh/GraphGAN/Pre-Train Embeddings/Struc2Vec_embeddings.emb\"\n",
    "index = np.array(G.nodes()).reshape(-1, 1)\n",
    "embedding_matrix = np.hstack([index, embeddings])\n",
    "embedding_list = embedding_matrix.tolist()\n",
    "embedding_str = [str(int(emb[0])) + \" \" + \" \".join([str(x) for x in emb[1:]]) + \"\\n\"\n",
    "                  for emb in embedding_list]\n",
    "with open(embedding_filename, \"w+\") as f:\n",
    "    lines = [str(5119) + \"\\t\" + str(50) + \"\\n\"] + embedding_str\n",
    "    f.writelines(lines)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d006cd09a7ebbb48eee4e86342fa19e01d16df56cb7967e71afee02ac2f94235"
  },
  "kernelspec": {
   "display_name": "Python 3.8.18 64-bit ('graphgan')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
